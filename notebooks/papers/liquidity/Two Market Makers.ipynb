{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Market Makers\n",
    "\n",
    "This notebook corresponds to section 4 (**Agent based models**) of \"Market Based Mechanisms for Incentivising Exchange Liquidity Provision\" available [here](https://vega.xyz/papers/liquidity.pdf). It models two market makers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, sys \n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse import diags\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from os import path\n",
    "count = 0\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1;\n",
    "gamma = 0.1\n",
    "\n",
    "sigma0 = 0.1\n",
    "sigma1 = 0.1\n",
    "lambd = 0.1\n",
    "r = 0.02\n",
    "rRisk0 = 0.01 \n",
    "rRisk1 = 0.01\n",
    "\n",
    "# This is the key; how does instantenaous trading volume react \n",
    "# to market making stake\n",
    "# and to fees\n",
    "\n",
    "def fee_volume_response(f):\n",
    "    f = np.maximum(f, np.zeros(np.size(f)))\n",
    "    f = np.minimum(f, np.ones(np.size(f)))\n",
    "    return 1.0/(f+0.001) - f\n",
    "\n",
    "\n",
    "\n",
    "def stake_volume_response(S):\n",
    "    return 1.0 / (1+np.exp(-0.05*S+2)) - 1.0 / (1+np.exp(2))\n",
    "\n",
    "# The actual trading and slashing\n",
    "# Agent index is still an input as they know their cost of capital\n",
    "# def running_gain_true(t,f,S,agent_idx):\n",
    "#     rRisk = (agent_idx == 0) * rRisk0 + (agent_idx == 1) * rRisk1\n",
    "#     return np.exp(-r*t) * (f * trading_volume_true(f,S) - lambd * sigma_true * S) - np.exp(rRisk*t)*S\n",
    "\n",
    "# Agents' assumption about trading volume\n",
    "# S should be vector of all agents' current stake\n",
    "def trading_volume_agt(f,S,agent_idx):\n",
    "    N_max = (agent_idx == 0) * 20000 + (agent_idx == 1) * 20000 \n",
    "    total_S = np.sum(S) \n",
    "    if total_S <= 0: \n",
    "        return 0\n",
    "    return  (S[agent_idx]/total_S ) * N_max * fee_volume_response(f) * stake_volume_response(total_S)\n",
    "\n",
    "# Running gain assumption by various agents\n",
    "# S should be vector of all agents' current stake\n",
    "def running_gain_agt(t,f,S,agent_idx):\n",
    "    return (agent_idx == 0) * (np.exp(-r*t) * (f * trading_volume_agt(f,S,agent_idx) - lambd * sigma0 * S[agent_idx])  - np.exp(rRisk0*t)*S[agent_idx]) \\\n",
    "            + (agent_idx == 1) * (np.exp(-r*t) * (f * trading_volume_agt(f,S,agent_idx) - lambd * sigma1 * S[agent_idx]) - np.exp(rRisk1*t)*S[agent_idx])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_span = np.linspace(0,1, 200) \n",
    "y = fee_volume_response(x_span)\n",
    "plt.xlabel('fee in %')\n",
    "plt.ylabel('volume in %')\n",
    "plt.title('Fee response')\n",
    "plt.plot(x_span,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_span = np.linspace(0,200, 200) \n",
    "y = stake_volume_response(x_span)\n",
    "plt.xlabel('stake')\n",
    "plt.ylabel('volume in %')\n",
    "plt.title('Stake response')\n",
    "plt.plot(x_span,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretization parameter initialization \n",
    "\n",
    "L_S = 120;\n",
    "L_f = 1;\n",
    "\n",
    "N_T = 5; delta_t = T / (N_T-1);\n",
    "N_S = 15; \n",
    "N_f = 15; \n",
    "\n",
    "t_span = np.linspace(0,T, N_T)\n",
    "f_span = np.linspace(0, L_f, N_f)\n",
    "S_span = np.linspace(0, L_S, N_S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control space definition     \n",
    "\n",
    "m = 3\n",
    "controls_lookup = np.matrix([\n",
    "                     [0,-m], \n",
    "                     [1,0], \n",
    "                     [2,m],\n",
    "])        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_idx_from(S,S_span):\n",
    "    min_S = S_span[0]\n",
    "    N_S = np.size(S_span)\n",
    "    max_S = S_span[N_S-1]\n",
    "    delta_S = (max_S-min_S)/(N_S-1)\n",
    "    return max(min(int(round(S/delta_S)), N_S-1),0)\n",
    "\n",
    "\n",
    "def update_value_at_time_idx(t_idx, a_vals, v_vals_old, v_vals_new, agent_idx):\n",
    "    for f_idx in range(0,N_f):\n",
    "        for S_idx0 in range(0,N_S):\n",
    "            for S_idx1 in range(0,N_S):\n",
    "                # if both want to increase stake then the fee should go down\n",
    "                # if both want to decrease stake then the fee should go up \n",
    "                # otherwise (only one is changing stake) fee stays the same\n",
    "                a_0 = a_vals[0, t_idx, f_idx, S_idx0, S_idx1]\n",
    "                a_1 = a_vals[1, t_idx, f_idx, S_idx0, S_idx1]\n",
    "                effective_a = (a_0 > 0) * (a_1 > 0) * a_0 + (a_0 < 0) * (a_1 < 0) * a_0\n",
    "            \n",
    "                f_new = f_span[f_idx] - gamma * effective_a * delta_t \n",
    "                S_new0 = S_span[S_idx0] + a_0 * delta_t \n",
    "                S_new1 = S_span[S_idx1] + a_1 * delta_t \n",
    "                t_new = t_span[t_idx] \n",
    "            \n",
    "                f_new_idx = grid_idx_from(f_new,f_span)\n",
    "                \n",
    "                S_new_idx0 = grid_idx_from(S_new0,S_span)\n",
    "                S_new_idx1 = grid_idx_from(S_new1,S_span)\n",
    "                S_new = np.array([S_new0,S_new1])\n",
    "                v_vals_new[agent_idx, t_idx, f_idx, S_idx0, S_idx1] = running_gain_agt(t_new, f_new, S_new, agent_idx)*delta_t \\\n",
    "                        + v_vals_old[agent_idx, t_idx+1, f_new_idx, S_new_idx0, S_new_idx1]\n",
    "\n",
    "\n",
    "def update_value(a_vals, v_vals_old, v_vals_new, agent_idx):\n",
    "    for t_idx in reversed(range(0,N_T-1)):\n",
    "        update_value_at_time_idx(t_idx, a_vals, v_vals_old, v_vals_new, agent_idx)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# When agents are optimizing their actions they need to use their assumptions\n",
    "def calculate_one_step_gain(t_idx, f_idx, S_idx0, S_idx1, a_vals, v_vals, agent_idx):\n",
    "    t_new = t_span[t_idx] \n",
    "    gains = np.zeros(np.size(controls_lookup[:,1]))\n",
    "    \n",
    "    # Agent 0 first\n",
    "    if agent_idx == 0:\n",
    "        for a0_idx in range(0,np.size(controls_lookup[:,1])):\n",
    "            a0 = controls_lookup[a0_idx,1]\n",
    "            # Other agent i.e. agent 1 is doing what their policy says\n",
    "            a1 = a_vals[1, t_idx, f_idx, S_idx0, S_idx1]\n",
    "            a_f = (a0 > 0) * (a1 > 0) * a0 + (a0 < 0) * (a1 < 0) * a0\n",
    "            f_new = f_span[f_idx] - gamma * a_f * delta_t \n",
    "            S_new0 = S_span[S_idx0] + a0 * delta_t \n",
    "            S_new1 = S_span[S_idx1] + a1 * delta_t \n",
    "            S_new = np.array([S_new0,S_new1])\n",
    "            \n",
    "            f_new_idx = int(max(min(f_idx - a_f, N_f-1),0))\n",
    "            S_new_idx0 = int(max(min(S_idx0 + a0, N_S-1),0))\n",
    "            S_new_idx1 = int(max(min(S_idx1 + a1, N_S-1),0))\n",
    "            \n",
    "            gains[a0_idx] = running_gain_agt(t_new, f_new, S_new, agent_idx)*delta_t \\\n",
    "                            + v_vals[agent_idx,t_idx+1, f_new_idx, S_new_idx0, S_new_idx1]\n",
    "        return gains\n",
    "\n",
    "    # Agent 1 next is the same as above except the roles are reversed\n",
    "    else:\n",
    "        for a1_idx in range(0,np.size(controls_lookup[:,1])):\n",
    "            a1 = controls_lookup[a1_idx,1]\n",
    "            # Other agent i.e. agent 1 is doing what their policy says\n",
    "            a0 = a_vals[0, t_idx, f_idx, S_idx0, S_idx1]\n",
    "            a_f = (a0 > 0) * (a1 > 0) * a0 + (a0 < 0) * (a1 < 0) * a0\n",
    "            f_new = f_span[f_idx] - gamma * a_f * delta_t \n",
    "            S_new0 = S_span[S_idx0] + a0 * delta_t \n",
    "            S_new1 = S_span[S_idx1] + a1 * delta_t \n",
    "            S_new = np.array([S_new0,S_new1])\n",
    "            \n",
    "            f_new_idx = int(max(min(f_idx - a_f, N_f-1),0))\n",
    "            S_new_idx0 = int(max(min(S_idx0 + a0, N_S-1),0))\n",
    "            S_new_idx1 = int(max(min(S_idx1 + a1, N_S-1),0))\n",
    "            \n",
    "            \n",
    "            gains[a1_idx] = running_gain_agt(t_new, f_new, S_new, agent_idx)*delta_t \\\n",
    "                            + v_vals[agent_idx,t_idx+1, f_new_idx, S_new_idx0, S_new_idx1]\n",
    "        return gains\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "def update_policy_at_time_idx(t_idx, v_vals, a_vals, agent_idx):\n",
    "    for f_idx in range(0,N_f):\n",
    "        for S_idx0 in range(0,N_S):\n",
    "            for S_idx1 in range(0,N_S):\n",
    "                gains = calculate_one_step_gain(t_idx, f_idx, S_idx0, S_idx1, a_vals, v_vals, agent_idx)\n",
    "                idx_maximizing = np.argmax(gains)\n",
    "                a_vals[agent_idx, t_idx, f_idx, S_idx0, S_idx1] = controls_lookup[idx_maximizing,1]\n",
    "        \n",
    "def update_policy(v_vals, a_vals, agent_idx):\n",
    "    for t_idx in reversed(range(0,N_T-1)):\n",
    "        update_policy_at_time_idx(t_idx, v_vals, a_vals, agent_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 10\n",
    "max_err = 3/(N_f*N_S)\n",
    "\n",
    "v_vals_new = np.zeros((2, np.size(t_span),np.size(f_span), np.size(S_span), np.size(S_span)))\n",
    "v_vals_old = np.zeros((2, np.size(t_span),np.size(f_span), np.size(S_span), np.size(S_span)))\n",
    "a_vals = np.ones((2, np.size(t_span),np.size(f_span), np.size(S_span), np.size(S_span)))\n",
    "a_vals_old = np.ones((2, np.size(t_span),np.size(f_span), np.size(S_span), np.size(S_span)))\n",
    "for iter_idx in range(0,max_iter):\n",
    "    # update agents' value functions with given control\n",
    "    update_value(a_vals, v_vals_old, v_vals_new, 0)\n",
    "    update_value(a_vals, v_vals_old, v_vals_new, 1)\n",
    "    \n",
    "    # make a copy of policy response for comparison purposes\n",
    "    a_vals_old = np.copy(a_vals)\n",
    "    \n",
    "    # update agents' policies\n",
    "    update_policy(v_vals_new, a_vals, 0)\n",
    "    update_policy(v_vals_new, a_vals, 1)\n",
    "    \n",
    "    # calculate how much we are still changing\n",
    "    diff_pol = np.sum(np.abs(a_vals_old - a_vals))/(N_f*N_S)\n",
    "    \n",
    "    v_vals_old = np.copy(v_vals_new)\n",
    "        \n",
    "    if (diff_pol < 1e-8):\n",
    "        print('Converged; change in policy fn. {0:1.5e}'.format(diff_pol))\n",
    "        break\n",
    "    else:\n",
    "        print('Will run another iteration; change in policy fn. {0:1.5e}'.format(diff_pol))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGridX, plotGridY = np.meshgrid(S_span, f_span)\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(projection='3d')\n",
    "surf = ax1.plot_surface(plotGridX, plotGridY, v_vals_new[1,0,:,:,0],linewidth=0, cmap=cm.jet, antialiased=True)\n",
    "ax1.set_xlabel('stake')\n",
    "ax1.set_ylabel('fee')\n",
    "ax1.set_zlabel('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGridX, plotGridY =  np.meshgrid(S_span, f_span)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "surf = ax1.contourf(plotGridX, plotGridY, a_vals[0,0,:,14,:], cmap=cm.jet, antialiased=True)\n",
    "ax1.set_xlabel('stake')\n",
    "ax1.set_ylabel('fee')\n",
    "ax1.set_zlabel('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stakes0 = np.zeros(N_T+1)\n",
    "stakes1 = np.zeros(N_T+1)\n",
    "stakes0[0] = 10\n",
    "stakes1[0] = 10\n",
    "fees = np.zeros(N_T+1)\n",
    "fees[0] = 0.1\n",
    "\n",
    "actions = np.zeros(N_T+1)\n",
    "for i in range(0,N_T):\n",
    "    f_idx = grid_idx_from(fees[i],f_span)\n",
    "    S_idx0 = grid_idx_from(stakes0[i],S_span)\n",
    "    S_idx1 = grid_idx_from(stakes1[i],S_span)\n",
    "    \n",
    "    a_0 = a_vals[0, i, f_idx, S_idx0, S_idx1]\n",
    "    a_1 = a_vals[1, i, f_idx, S_idx0, S_idx1]\n",
    "    a_fee = (a_0 > 0) * (a_1 > 0) * a_0 + (a_0 < 0) * (a_1 < 0) * a_0 \n",
    "    \n",
    "\n",
    "    fees[i+1] = fees[i] - gamma * a_fee * delta_t\n",
    "    stakes0[i+1] = stakes0[i] + a_0 * delta_t\n",
    "    stakes1[i+1] = stakes1[i] + a_1 * delta_t\n",
    "\n",
    "plt.plot(t_span, stakes0[0:N_T])\n",
    "plt.plot(t_span, stakes1[0:N_T])\n",
    "plt.title('Stake evolution')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('stake')\n",
    "fig = plt.figure()\n",
    "plt.plot(t_span, fees[0:N_T])\n",
    "plt.title('Fees evolution')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('fees')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_span = np.linspace(0,1, 200) \n",
    "y = fee_volume_response(x_span)\n",
    "plt.xlabel('fee in %')\n",
    "plt.ylabel('volume in %')\n",
    "plt.plot(x_span,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[100]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
